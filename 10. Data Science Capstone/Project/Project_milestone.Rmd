---
title: "Milestone Report"
author: "Yuchan Jeong"
date: "`r Sys.Date()`"
output: html_document
---

```{r package, error = FALSE, warning = FALSE, message = FALSE}
library(stringr)
library(knitr)
library(ggplot2)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(quanteda, warn.conflicts=F)
library(quanteda.textplots)
```


This Milestone Report details the Exploratory Data Analysis (EDA) performed on the SwiftKey data corpus, the foundational step for developing a next-word prediction algorithm. The primary goal is to understand the major features of the data through basic summary statistics and visualizations, and to outline the plan for the final prediction algorithm and Shiny application.

```{r, error = FALSE, warning = FALSE}
setwd("C:/Users/PC/Documents/GitHub/datasciencecoursera/10. Data Science Capstone/Project")

if (isFALSE(("SwiftKey.zip" %in% list.files(getwd())))) {
  url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
      download.file(url, file.path(getwd(), "SwiftKey.zip"))
      unzip(zipfile = "SwiftKey.zip")
      setwd("C:/Users/PC/Documents/GitHub/datasciencecoursera/10. Data Science Capstone/Project/final/en_US")
      en_US_blogs <- readLines("en_US.blogs.txt")
      en_US_twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8")
      en_US_news <- readLines("en_US.news.txt", encoding = "UTF-8")
} else setwd("C:/Users/PC/Documents/GitHub/datasciencecoursera/10. Data Science Capstone/Project/final/en_US"); en_US_blogs <- readLines("en_US.blogs.txt"); en_US_twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8"); en_US_news <- readLines("en_US.news.txt", encoding = "UTF-8")
```
## Exploratory Data Analysis for Line Count
```{r}
blogs_lines <- length(en_US_blogs)
news_lines <- length(en_US_news)
twitter_lines <- length(en_US_twitter)

line_counts <- c(
  blogs = blogs_lines,
  news = news_lines,
  twitter = twitter_lines
)

print(format(line_counts, big.mark = ","))
```
## Exploratory Data Analysis for Longest Line Length
```{r}
blogs_max_len <- max(nchar(en_US_blogs, type = "bytes"))
news_max_len <- max(nchar(en_US_news, type = "bytes"))
twitter_max_len <- max(nchar(en_US_twitter, type = "bytes"))

max_line_lengths <- c(
  blogs = blogs_max_len,
  news = news_max_len,
  twitter = twitter_max_len
)

print(format(max_line_lengths, big.mark = ","))
```

## Exploratory Data Analysis for Word Count
```{r}
approx_word_count <- function(text_data) {
  sum(str_count(text_data, " ") + 1)
}

blogs_word_count <- approx_word_count(en_US_blogs)
news_word_count <- approx_word_count(en_US_news)
twitter_word_count <- approx_word_count(en_US_twitter)

total_word_counts <- c(
  blogs = blogs_word_count,
  news = news_word_count,
  twitter = twitter_word_count
)
print(format(total_word_counts, big.mark = ","))
```
## Summary Table
```{r}
data.frame(
  Source = c("Blogs", "News", "Twitter"),
  Line_Count = line_counts,
  Max_Line_Length = max_line_lengths,
  Approx_Word_Count = total_word_counts
)
```

## Plot
```{r, warning = FALSE}
# Function to calculate word count per line
word_count_per_line <- function(data) {
    # Using stringr::str_count to count non-space sequences as words
    str_count(data, "\\S+")
}

# Calculate word counts for a sample (since the full dataset is huge)
set.seed(42) # For reproducibility
sample_size <- 10000 # Use a smaller sample for faster plotting

blogs_wc <- word_count_per_line(sample(en_US_blogs, sample_size))
news_wc <- word_count_per_line(sample(en_US_news, sample_size))
twitter_wc <- word_count_per_line(sample(en_US_twitter, sample_size))

# Combine into a data frame for ggplot
wc_data <- data.frame(
    Source = rep(c("Blogs", "News", "Twitter"), 
                 times = c(length(blogs_wc), length(news_wc), length(twitter_wc))),
    Word_Count = c(blogs_wc, news_wc, twitter_wc)
)

# Plot the histogram
ggplot(wc_data, aes(x = Word_Count, fill = Source)) +
    geom_histogram(binwidth = 10, position = "identity", alpha = 0.6) +
    xlim(0, 150) + # Limit x-axis for better visibility of the main distributions
    facet_wrap(~Source, scales = "free_y") +
    labs(title = "Distribution of Line Lengths (Words per Line)",
         x = "Word Count per Line",
         y = "Frequency") +
    theme_minimal()
```

```{r, warning = FALSE, echo = FALSE}
# This is from Cesine's github. We download bad-words from github.
setwd("C:/Users/PC/Documents/GitHub/datasciencecoursera/10. Data Science Capstone/Project")

if (isFALSE(("bad_words" %in% list.files(getwd())))) {
  url_bad_words <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
 download.file(url_bad_words, file.path(getwd(), "en.txt"))
 bad_words <- readLines("en.txt")
 } else bad_words <- readLines("en.txt")
```
```{r}
set.seed(42)
sample_data <- c(sample(en_US_blogs, length(en_US_blogs) * 0.1),
                 sample(en_US_news, length(en_US_news) * 0.1),
                 sample(en_US_twitter, length(en_US_twitter) * 0.1))

sample_data <-iconv(sample_data,"latin1","ASCII",sub="")
Mycorpus <- corpus(sample_data)

Mytokens <- tokens(Mycorpus, what = "word", remove_url = TRUE,
                   remove_punc = TRUE, remove_numbers = TRUE, 
                   remove_symbols = TRUE, remove_separators = TRUE) %>%
            tokens_tolower() %>% 
            tokens_remove(bad_words, valuetype='fixed') 
```
```{r}
dfm.1gram <- Mytokens %>% tokens_ngrams(n = 1, concatenator = " ") %>% dfm()
dfm.2gram <- Mytokens %>% tokens_ngrams(n = 2, concatenator = " ") %>% dfm()
dfm.3gram <- Mytokens %>% tokens_ngrams(n = 3, concatenator = " ") %>% dfm()
```

```{r}
par(mar=c(5,10,4,4))
barplot(sort(topfeatures(dfm.1gram,20)),
        xlab='count',cex.name=0.7,horiz=TRUE,las=1,main='Top20 1-gram')
```



```{r}
barplot(sort(topfeatures(dfm.2gram,20)),
        xlab='count',cex.name=0.7,horiz=TRUE,las=1,main='Top20 2-gram')
```

```{r}
barplot(sort(topfeatures(dfm.3gram,20)),
        xlab='count',cex.name=0.7,horiz=TRUE,las=1,main='Top20 3-gram')
```

## Word Cloud

```{r, message = FALSE, warning = FALSE}
par(mfrow = c(1, 3))

# 1-gram word cloud
textplot_wordcloud(dfm.1gram, max_words = 20, min_count = 6, random_order = FALSE, rotation = .25, color = RColorBrewer::brewer.pal(8, "Dark2"), main = "Top 100 1-gram")

# 2-gram word cloud
textplot_wordcloud(dfm.2gram, max_words = 20, min_count = 6, random_order = FALSE, rotation = .25, color = RColorBrewer::brewer.pal(8, "Dark2"), main = "Top 100 2-gram")

# 3-gram word cloud
textplot_wordcloud(dfm.3gram, max_words = 20, min_count = 6, random_order = FALSE, rotation = .25, color = RColorBrewer::brewer.pal(8, "Dark2"), main = "Top 100 3-gram")
```